---
title: Configuring MQ on a VM
description: Configuring MQ on a VM

---

To support Kubernetes, changes to SPM were required.
For more information on these changes, see
[Kubernetes Architecture](https://www.ibm.com/support/knowledgecenter/SS8S5A_7.0.10/com.ibm.curam.wlp.doc/Kubernetes/c_KubArchitecture.html) section in
the Knowledge Center.

When SPM is containerized on Kubernetes, it uses IBM® MQ to manage JMS messages for Cúram Deferred Processes and Cúram Workflows. IBM MQ is a queue managing service from the IBM stack. For more information about MQ, see [About IBM MQ](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q001010_.htm).

If MQ fails, certain functionality will be unusable. This includes and is not limited to, creation of a case and creation of an application.

The MQ Cluster set up requires two queue managers nodes, with one active/primary queue manager, and one standby/secondary queue manager.

Social Program Management supports only IBM MQ LTS on a VM. The steps below outline how to do this. In this runbook we will outline the steps to create:

* [Queues](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003090_.htm)
* [Listeners](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003300_.htm)
* [Channels](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003220_.htm)
* [Topics](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003320_.htm)

<InlineNotification>

**Note:** The MQ version for this runbook verification is  9.1.0 LTS.

</InlineNotification>

For the runbook, two standalone VMs were used as MQ nodes.

### Queue manager names

For runbook configuration, the following naming conversion was used throughout the MQ setup: QM_NamingConvention_AppName. This must be unique, but ensure you change the commands used on this page accordingly.

**Queue Name:**

* QM_minikube_curam

**Channel Name:** This value should be all capitals

CHL_NamingConvention_AppName

* CHL_MINIKUBE_CURAM

**Listeners Name:** This value should be all capitals

LS_NamingConvention_AppName

* LS_MINIKUBE_CURAM

## MQ stages

On both MQ nodes run the following command as root:

```shell
su - mqm # Changing user into mqm
export PATH=/opt/mqm/inst1/bin:$PATH
```

<InlineNotification kind="warning">

**Important!**

Run the export PATH command on both MQ nodes, this command will be used in further commands in the runbook.

</InlineNotification>

### Shared storage

Create the shared storage for our nodes.

<InlineNotification>

**Note:** Run the commands as root.

</InlineNotification>

On the shared node run the following commands:

```shell
mkdir -p /MQHA/logs
mkdir -p /MQHA/qmgrs
mkdir -p /MQHA/scratch
useradd mqha -s /sbin/nologin
chown -R mqha:mqha /MQHA/*
```

Verify that the UID and GUID match the owner ID by running the following command:

```shell
echo "/MQHA  MQ.FQDN(rw,sync,no_wdelay,fsid=0,anonuid=1001,anongid=1001)" >> /etc/exports
```

Start and enable both the nfs service and rpcbind service by running the following commands:

```shell
systemctl start nfs-server.service
systemctl enable nfs-server.service
systemctl start rpcbind
systemctl enable rpcbind
```

On MQ nodes run the following commands:

<InlineNotification>

**Note:** Commands to be run as root.

</InlineNotification>

```shell
echo "SHAREDNODEADDRESS:/MQHA  /MQHA  nfs  defaults  0 0" >> /etc/fstab
systemctl start rpcbind
systemctl enable rpcbind
mkdir -p /MQHA
chmod 1777 /MQHA #Check permissions
mount /MQHA
```

### Create QMs

When creating the queue, start on the secondary node first then move to the primary node.

On the secondary MQ node, run the following commands:

```shell
crtmqm -ld /MQHA/logs -md /MQHA/qmgrs QM_minikube_curam
dspmqinf -o command QM_minikube_curam
```

Save the output of the above command. It should look like the following.

```shell
addmqinf -s QueueManager -v Name=QM_minikube_curam -v Directory=QM_minikube_curam -v Prefix=/var/mqm -v DataPath=/MQHA/qmgrs/QM_minikube_curam
```

Wait for /MQHA/qmgrs/QM_minikube_curam/qm.ini to appear on other node

On the primary MQ node run the following commands:

```shell
addmqinf -s QueueManager -v Name=QM_minikube_curam -v Directory=QM_minikube_curam -v Prefix=/var/mqm -v DataPath=/MQHA/qmgrs/QM_minikube_curam
strmqm -x QM_minikube_curam
```

On the secondary MQ node run the following command:

```shell
strmqm -x QM_minikube_curam
```

### Create queues

On the primary MQ node run the following commands:

```shell
runmqsc QM_minikube_curam <<-EOS
DEFINE QLOCAL(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)
DEFINE QLOCAL(QN.WORKFLOWERROR) BOTHRESH(5) BOQNAME(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)
DEFINE QLOCAL(QN.WORKFLOWENACTMENT) BOTHRESH(5) BOQNAME(QN.WORKFLOWERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)
DEFINE QLOCAL(QN.WORKFLOWACTIVITY) BOTHRESH(5) BOQNAME(QN.WORKFLOWERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)
DEFINE QLOCAL(QN.DPERROR) BOTHRESH(5) BOQNAME(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)
DEFINE QLOCAL(QN.DPENACTMENT) BOTHRESH(5) BOQNAME(QN.DPERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)
ALTER QMGR CHLAUTH(DISABLED)
ALTER QMGR DEADQ(QN.CURAMDEADMESSAGEQUEUE)
EOS
```

### Create listeners

On the primary MQ node run the following commands:

```shell
runmqsc QM_minikube_curam <<-EOS
DEFINE LISTENER (LS_MINIKUBE_CURAM) TRPTYPE (TCP) CONTROL (QMGR) PORT (1414)
START LISTENER (LS_MINIKUBE_CURAM)
EOS
```

### Create channels

On the primary MQ node run the following command:

* Enter your MQ node names into the commands below

<InlineNotification>

**Note:** CERTLABL expects the value to be lower case ibmwebspheremq + Queue Name
For this example it will be ibmwebspheremqqm_minikube_curam

</InlineNotification>

```shell
runmqsc QM_minikube_curam <<-EOS
DEFINE CHANNEL(CHL_MINIKUBE_CURAM) CHLTYPE(SVRCONN)  TRPTYPE(TCP) MCAUSER('mqm') SSLCIPH (TLS_RSA_WITH_AES_128_CBC_SHA256)  CERTLABL ('ibmwebspheremqqm_minikube_curam') SSLCAUTH (OPTIONAL) REPLACE
DEFINE CHANNEL(CHL_MINIKUBE_CURAM) CHLTYPE(CLNTCONN) TRPTYPE(TCP) CONNAME('Node1(1414),Node2(1414)') QMNAME(QM_minikube_curam) SSLCIPH (TLS_RSA_WITH_AES_128_CBC_SHA256) CERTLABL ('ibmwebspheremqqm_minikube_curam') REPLACE
EOS
```

### Create topics

On the primary MQ node run the following command:

```shell
runmqsc QM_minikube_curam <<-EOS
DEFINE TOPIC (CURAMCACHEINVALIDATIONTOPIC) TOPICSTR (CURAMCACHEINVALIDATIONTOPIC)
ALTER QMGR CONNAUTH('CHECK.PWD')
DEFINE AUTHINFO('CHECK.PWD') AUTHTYPE(IDPWOS) CHCKLOCL(OPTIONAL) CHCKCLNT(OPTIONAL)
EOS
```

### Configure security

The configuration of security is in four parts

* Setting the object type.
* Creating the keystore and certs.
* Updating the certs on both nodes.
* Refreshing security settings.

<InlineNotification>

**Note:** The application pods run as a non-root user (default). This non-root user must exist on both MQ nodes.

</InlineNotification>

On the secondary MQ node run the following command:

```shell
useradd -g 0 -M default && usermod -L default
```

On the primary MQ node run the following commands:

```shell
useradd -g 0 -M default && usermod -L default
runmqsc QM_minikube_curam <<-EOS
SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.DPENACTMENT') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.DPERROR') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWACTIVITY') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWENACTMENT') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWERROR') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.CURAMDEADMESSAGEQUEUE') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(LISTENER) PROFILE('LS_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(CHANNEL) PROFILE('CHL_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(CLNTCONN) PROFILE('CHL_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)
SET AUTHREC OBJTYPE(TOPIC) PROFILE('CURAMCACHEINVALIDATIONTOPIC') PRINCIPAL('default') AUTHADD(ALL)
EOS
```

```shell
runmqckm -keydb -create -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -type cms -pw Passw0rd -stash
runmqakm -cert -create -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -size 2048 -dn "CN=QM_minikube_curam,O=IBM,C=US" -x509version 3 -expire 365 -sig_alg SHA1WithRSA
runmqakm -cert -extract -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -target /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.arm
runmqakm -cert -export -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -target /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -target_type pkcs12 -target_pw Passw0rd
```

```shell
openssl pkcs12 -in /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -passin pass:Passw0rd -nocerts -nodes | sed -ne '/-BEGIN PRIVATE KEY-/,/-END PRIVATE KEY-/p' > /MQHA/qmgrs/QM_minikube_curam/ssl/tls.key
openssl pkcs12 -in /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -passin pass:Passw0rd -clcerts -nokeys | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /MQHA/qmgrs/QM_minikube_curam/ssl/tls.crt
```

```shell
runmqsc QM_minikube_curam <<-EOS
ALTER QMGR CONNAUTH('CHECK.PWD')
DEFINE AUTHINFO('CHECK.PWD') AUTHTYPE(IDPWOS) CHCKLOCL(OPTIONAL) CHCKCLNT(OPTIONAL)
REFRESH SECURITY TYPE(SSL)
REFRESH SECURITY TYPE(AUTHSERV)
REFRESH SECURITY TYPE(CONNAUTH)
EOS
```

After these stages have been run MQ should be configured.

### Clean up QMs/channels/listeners

Used these steps if you are reconfiguring MQ or cleaning up MQ.

On both MQ nodes run the following commands:

```shell
endmqm -w QM_minikube_curam
dltmqm QM_minikube_curam
rmvmqinf QM_minikube_curam
```

On either MQ node run the following commands:

```shell
rm -rf /MQHA/qmgrs/**
rm -rf /MQHA/logs/**
rm -rf /MQHA/scratch
endmqm -w QM_minikube_curam
dltmqm QM_minikube_curam
rmvmqinf QM_minikube_curam
```

### MQ on OpenShift

#### Stateful Sets

If a highly available MQ cluster is desired, a **Stateful Set** can be used. The stateful set used for SPM contains two identical
pods, one active pod and one standby pod. If the active pod goes down, the standby pod is moved into the active role and a new pod is rescheduled in standby mode.
This occurs seamlessly, with persistent storage allowing for minimal downtime. The Stateful Set used in SPM requires several values that must be configured prior to
deployment. These values are those located under the MQ `multiInstance` section within the relevant values file. There, NFS or Ceph can be chosen as the desired
multi-instance delivery method.

* **NFS** - In order to deploy with NFS, an NFS server and NFS folder must be available and configured; the supplementalGroups may need to be provided depending on the NFS server security setup.
* **Ceph** - In order to deploy with Ceph, the desired Storage Class must be provided.

#### Persistent Volumes & Persistent Volume Claims

A **PersistentVolume** (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.
It is a resource in the cluster just like a node is a cluster resource. A **PersistentVolumeClaim** (PVC) is a request for storage by a user. It is similar to
a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific
size and access modes.

When using NFS as the desired multi-instance method, the PV and PVCs must be configured by the user. Within the PVs, the NFS IP and NFS folder must be provided.
In the PV, a `claimRef` can be defined in order to ensure that the correct PVC matches with the correct PV. The templates provided also contain labels, which can
also be used to ensure correct coupling.

If using Ceph, the PVs are dynamically configured. Therefore, no further configuration is required.
